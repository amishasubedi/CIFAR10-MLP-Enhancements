{"cells":[{"cell_type":"markdown","metadata":{"id":"etUG4VY96GHj"},"source":["# Autoencoder with pytorch\n","\n","\n","\n","Autoencoders are a type of neural network used for unsupervised learning of efficient codings. The goal of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Unlike CNNs, which are trained to optimize for accuracy in classification tasks, autoencoders are designed to learn representations of the input data and then reconstruct the input data from these representations.\n","\n","Here, I'll the use of an autoencoder for CIFAR10 image classification. The hypothesis is that by learning efficient representations of CIFAR10 images, the network can achieve better classification accuracy (better than MLP and CNN), especially when coupled with a classifier on top of the encoded representations.\n"]},{"cell_type":"markdown","metadata":{"id":"UK7LvSAv6lQz"},"source":["#### Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qzs76hgM6C1R"},"outputs":[],"source":["# Load required libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{"id":"hkoDe-k8GvOa"},"source":["#### Image Preprocessing\n","\n","Before feeding the CIFAR10 images into the neural network, they need to be properly preprocessed, which is achieved using torchvision.transforms, which are common image transformations for PyTorch models.\n","\n","'transforms.Compose' is a method that bundles multiple transformations together.\n"," Here, two main transformations are applied:\n","   1. 'transforms.ToTensor()' converts PIL images or NumPy ndarrays into PyTorch tensors.\n","   2. 'transforms.Normalize()' normalizes the tensor image with mean and standard deviation.\n","\n"," In this case, we normalize all three color channels (R, G, B) with mean 0.5 and std 0.5.\n","\n","Additionally, a utility function 'imshow' is defined to visualize the images.\n","It takes a tensor image, unnormalizes it, converts it to a NumPy array, and then uses matplotlib to display the image\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgITJxyOUUxa"},"outputs":[],"source":["# image preprocessing\n","train_transform = transforms.Compose(\n","    [transforms.RandomHorizontalFlip(p = 0.5),\n","     transforms.RandomAffine(degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",")\n","\n","test_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",")\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sMZp6zJbUaJI"},"source":["#### Define the autoencoder architecture\n","\n","This section implements the Autoencoder architecture, a type of neural network used for unsupervised learning of efficient codings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpcPn6cXUb4b"},"outputs":[],"source":["class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 16, 3, stride=2, padding=1), # output: 16 x 16 x 16\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, 3, stride=2, padding=1), # output: 32 x 8 x 8\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 7) # output: 64 x 2 x 2\n","        )\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, 7),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 3, 3, stride=2, padding=1, output_padding=1),\n","            nn.Sigmoid() # output: 3 x 32 x 32\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"en7lr3NiUfQE"},"outputs":[],"source":["# Initialize the autoencoder\n","autoencoder = Autoencoder()\n","\n","# Define loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"FBsdFtoDHK3a"},"source":["#### Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5660,"status":"ok","timestamp":1701463093469,"user":{"displayName":"Amisha Subedi","userId":"07352502213288740018"},"user_tz":360},"id":"mOmWaDoIHO7Z","outputId":"32661a04-43c0-438e-d82c-cb05435f111d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 80156055.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"]}],"source":["batch_size = 4\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"toXqcLvdUjRI"},"source":["#### Training\n","\n","In the training of the autoencoder model, conducted over 10 epochs, we focused on optimizing the reconstruction of input images. Each epoch involved processing the training data in batches, using the loss between the input and the output images to iteratively update the model's weights via backpropagation and optimizer steps. The model's progress was monitored by calculating and reporting the average loss per epoch, providing insights into the effectiveness of the reconstruction process. This continuous refinement culminated in the successful completion of the autoencoder's training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"cyvpMzVrUkj0","outputId":"24fc5cc0-974c-4340-b60d-acbfd82e4508"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.262\n","Epoch 2, Loss: 0.243\n","Epoch 3, Loss: 0.242\n","Epoch 4, Loss: 0.241\n","Epoch 5, Loss: 0.240\n","Epoch 6, Loss: 0.239\n","Epoch 7, Loss: 0.239\n","Epoch 8, Loss: 0.239\n","Epoch 9, Loss: 0.238\n","Epoch 10, Loss: 0.238\n","Finished Training Autoencoder\n"]}],"source":["# Training loop\n","for epoch in range(10):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, _ = data\n","        optimizer.zero_grad()\n","        outputs = autoencoder(inputs)\n","        loss = criterion(outputs, inputs)  # Reconstruction loss\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader):.3f}')\n","\n","print('Finished Training Autoencoder')"]},{"cell_type":"markdown","metadata":{"id":"YwkXfHNNGLRV"},"source":["#### Performance Testing\n","\n","To evaluate the accuracy of the autoencoder on the CIFAR10 dataset, we need to measure how well the autoencoder reconstructs the input images. Since autoencoders are typically used for tasks like dimensionality reduction or feature learning rather than classification, traditional accuracy metrics (like those used in classification tasks) are not directly applicable. Instead, we'll use reconstruction error to measure performance. For this purpose, Mean Squared Error (MSE) is commonly used."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1701466048497,"user":{"displayName":"Amisha Subedi","userId":"07352502213288740018"},"user_tz":360},"id":"jO9oEQ-HGbKx","outputId":"f9c63073-d179-4945-ea9f-7853b5393639"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"execute_result","data":{"text/plain":["Autoencoder(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (3): ReLU()\n","    (4): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n","  )\n","  (decoder): Sequential(\n","    (0): ConvTranspose2d(64, 32, kernel_size=(7, 7), stride=(1, 1))\n","    (1): ReLU()\n","    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (3): ReLU()\n","    (4): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (5): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":8}],"source":["# Load the test dataset\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","# Switch the model to evaluation mode\n","autoencoder.eval()"]},{"cell_type":"code","source":["# Initialize the loss and class-wise loss tracking\n","total_loss = 0.0\n","class_losses = {i: 0.0 for i in range(10)}  # 10 classes in CIFAR10\n","class_counts = {i: 0 for i in range(10)}\n","\n","# No gradient is needed for evaluation\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = autoencoder(images)\n","        loss = criterion(outputs, images)\n","        total_loss += loss.item() * images.size(0)\n","\n","        # Compute class-wise loss\n","        for label in labels:\n","            class_losses[label.item()] += loss.item()\n","            class_counts[label.item()] += 1\n","\n","# Calculate the average loss\n","avg_loss = total_loss / len(testset)\n","print(f'Overall Mean Squared Error: {avg_loss:.4f}')\n","\n","# Print class-wise MSE\n","for i in range(10):\n","    avg_class_loss = class_losses[i] / class_counts[i]\n","    print(f'Class {i} Mean Squared Error: {avg_class_loss:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFhYPPnXR-ei","executionInfo":{"status":"ok","timestamp":1701466090418,"user_tz":360,"elapsed":13881,"user":{"displayName":"Amisha Subedi","userId":"07352502213288740018"}},"outputId":"c34ec326-bc2e-4ce4-f9b6-b2653938dd32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overall Mean Squared Error: 0.1439\n","Class 0 Mean Squared Error: 0.1300\n","Class 1 Mean Squared Error: 0.1524\n","Class 2 Mean Squared Error: 0.1394\n","Class 3 Mean Squared Error: 0.1495\n","Class 4 Mean Squared Error: 0.1434\n","Class 5 Mean Squared Error: 0.1469\n","Class 6 Mean Squared Error: 0.1532\n","Class 7 Mean Squared Error: 0.1438\n","Class 8 Mean Squared Error: 0.1355\n","Class 9 Mean Squared Error: 0.1451\n"]}]},{"cell_type":"markdown","source":["### Conclusion\n","\n","The training of the autoencoder model on the CIFAR dataset culminated in an overall mean squared error (MSE) of 0.1439, indicating a competent level of image reconstruction. The MSE varied across classes, with the lowest being 0.1300 for Class 0 and the highest at 0.1532 for Class 6, reflecting the model's varying proficiency in reconstructing different types of images. These results demonstrate the autoencoder's general effectiveness in capturing and reconstructing the dataset's diverse image features, while also suggesting the potential for further model optimization to achieve even lower reconstruction errors across all classes."],"metadata":{"id":"lNNq7I6lfdjo"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbLErOzQlNqi0Vjtr6RnhY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}